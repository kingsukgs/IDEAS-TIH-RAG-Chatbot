# -*- coding: utf-8 -*-
"""IDEAS-TIH-RAG-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OMsSEeFsx00zxNi2pbA4ER5z8N-VpB-x
"""

!pip install transformers accelerate torch sentencepiece faiss-cpu beautifulsoup4 requests tqdm --quiet

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

import numpy as np
import faiss

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer

# List of webpages to be scraped
urls = [
    "https://www.ideas-tih.org/",
    "https://www.ideas-tih.org/about-us",
    "https://www.ideas-tih.org/activities",
    "https://www.ideas-tih.org/s-projects-basic",
    "https://www.ideas-tih.org/events",
    "https://www.ideas-tih.org/career",
    "https://www.ideas-tih.org/media",
    "https://www.ideas-tih.org/internship",
    "https://www.ideas-tih.org/education",
    "https://www.ideas-tih.org/staff",
    "https://www.ideas-tih.org/staff",
    "https://www.ideas-tih.org/autumninternship2025",
    "https://www.isical.ac.in/about/about-institute",
    "https://www.isical.ac.in/content/timeline-0",
    "https://www.linkedin.com/company/ideastih/about/",
    "https://en.wikipedia.org/wiki/Indian_Statistical_Institute",
    "https://www.ideas-tih.org/initiatives"
]

def scrape_url(url):
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        text = soup.get_text(separator=" ", strip=True)
        print(f"‚úÖ Scraped: {url} (chars: {len(text)})")
        return text
    except Exception as e:
        print(f"‚ö†Ô∏è Failed: {url} ‚Äî {e}")
        return ""

corpus = "\n\n".join([scrape_url(u) for u in urls])
print("\nüìö Total corpus size:", len(corpus), "characters")

def chunk_text(text, chunk_size=600):
    words = text.split()
    chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

chunks = chunk_text(corpus)
print(f"üìÑ Total Chunks: {len(chunks)}")

embed_model = SentenceTransformer("all-MiniLM-L6-v2")

print("üîç Generating embeddings...")
embeddings = embed_model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)

embeddings = np.array(embeddings).astype("float32")
print("‚úÖ Embeddings shape:", embeddings.shape)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

print("üéâ FAISS index built successfully!")
print("üì¶ Total vectors stored:", index.ntotal)

import torch

model_name = "Qwen/Qwen2.5-1.5B-Instruct"

print("‚è≥ Loading Qwen2.5-1.5B (this may take 20‚Äì40 seconds)...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

gen = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.4
)

print("Qwen2.5-1.5B loaded successfully!")

def retrieve(query, k=4):
    # Convert query ‚Üí embedding ‚Üí FAISS search
    q_emb = embed_model.encode([query], convert_to_numpy=True).astype("float32")
    distances, indices = index.search(q_emb, k)

    retrieved = []
    for i in indices[0]:
        if i < len(chunks):
            retrieved.append(chunks[i])
    return retrieved

def build_prompt(query):
    retrieved_chunks = retrieve(query)
    context = "\n\n".join(retrieved_chunks)

    prompt = f"""
You are **IDEAS-TIH Assistant**, an AI designed to provide accurate, factual, and concise information
based strictly on the available documents and website content of IDEAS-TIH (Institute of Data Engineering, Analytics and Science Foundation).

### Your Duties:
1. **Use ONLY the context provided below** to answer the user's question.
2. If relevant information is available, summarize it clearly and professionally.
3. If the context does not contain the required information, respond with:
   **"I could not find this information in the provided data."**
4. Do NOT make assumptions, invent details, or hallucinate.
5. Maintain a helpful, polite, and formal tone.
6. Include ONLY factual content from the context.

---

### CONTEXT EXTRACTED FROM IDEAS-TIH WEBSITE:
{context}

---

### USER QUESTION:
{query}

---

### ASSISTANT ANSWER (based strictly on the context above):
"""
    return prompt

def answer_query(query):
    prompt = build_prompt(query)
    full_generated_text = gen(prompt)[0]["generated_text"]

    answer_start_marker = "### ASSISTANT ANSWER (based strictly on the context above):"

    reply = ""
    if answer_start_marker in full_generated_text:
        reply = full_generated_text.split(answer_start_marker)[-1].strip()
    else:

        if full_generated_text.startswith(prompt):
            reply = full_generated_text[len(prompt):].strip()
        else:
            reply = full_generated_text.strip() # Last resort, return raw output

    return reply

print("\nü§ñ IDEAS-TIH RAG Chatbot Ready!")
print("Type 'exit' to quit.\n")

while True:
    query = input("üë§ You: ").strip()

    if query.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break

    try:
        reply = answer_query(query)
        print("ü§ñ Bot:", reply, "\n")
    except Exception as e:
        print("‚ö†Ô∏è Error:", e)